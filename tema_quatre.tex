\section{Variables Aleatòries Contínues}
\subsection{Mesures de probabilitat absolutament contínues. Funció de densitat}
Sigui $(\Omega, \mathcal{A},p)$ un espai de probabilitat, amb probabilitat induïda en $(\real, \mathcal{B},p_{X})$ (quan prenem la variable aleatòria $X$).

\begin{defi}
  Siguin$\mu_{1},\mu_{2}$ mesures sobre un espai de mesura $(X,\mathcal{A})$, diem que $\mu_{1}$ és \textbf{absolutament contínua} respecte $\mu_{2}$ ($\mu_{1}<<\mu_{2}$) si $\forall A\in \mathcal{A}$,
  \[
    \mu_{2}(A)=0 \implies \mu_{1}(A)=0
  \]
\end{defi}

\begin{defi}
  Una variable aleatòria $X$ és \textbf{absolutament contínua} (o contínua per abreujar), si $p_{X}<<\lambda$ ($\lambda$ és la mesura de Lebesgue).
\end{defi}

\begin{obs}
  Les variables aleatòries discretes \underline{no} són absolutament contínues: \\
  Si $p(X=a)=P_{a}>0$ i prenem $B=\setb{a}$, tenim $\lambda(\setb{a})=0$, però $p_{X}(\setb{a})=P_{a}>0$
\end{obs}

El teorema fonamental que ens permet traduir $p_{X}$ (si $X$ és absolutament contínua) a càlculs usant la mesura $\lambda$, és el següent:

\begin{thm}[(Radon-Nikodym)]
  Sigui $(X,\mathcal{A})$ un espai mesurable i $\mu_{1},\mu_{2}$ mesures sobre $(X,\mathcal{A})$ amb $\mu_{1}<<\mu_{2}$. Aleshores, existeix una funció $f_{\mu_{1}}$, $\mu_{2}$-mesurable tal que
  \[
    \forall A \in \mathcal{A}, \, \mu_{1}(A)=\int_{A}f_{\mu_{1}}d\mu_{2}
  \]
  A més, $f_{\mu_{1}}$ és única $\mu_{2}$-gairebé arreu
\end{thm}

\begin{defi}
  La funció $f_{\mu_{1}}$ és la \textbf{funció de densitat} de la mesura $\mu_{1}$ respecte a $\mu_{2}$. \\
  En el nostre context, $X$ és una variable aleatòria absolutament contínua, i
  \[
    \begin{rcases}
      (\real, \mathcal{B},\underbrace{\lambda}_{\mu_{2}})\\
      (\real, \mathcal{B}, \underbrace{p_{X}}_{\mu_{1}})
    \end{rcases}\underset{\mathclap{X abs. cont.}}{\implies} \, p_{X}<<\lambda\overset{R-N}{\implies} \forall B \in \mathcal{B}, \, \boxed{p_{X}(B)=\int_{B}f_{X}d\lambda}
  \]
\end{defi}

\begin{defi}
  La funció $f_{X}$ s'anomena \textbf{funció de densitat de probabilitat} de $X$.
\end{defi}

\begin{obs}
  En la literatura, s'escriu $f_{\mu_{1}}=\dfrac{d\mu_{1}}{d\mu_{2}}$ (Derivada de Radon-Nikodym).
\end{obs}

\newpage

\begin{prop}
  Si $X$ és una variable aleatòria absolutament contínua, amb funció de densitat de probabilitat $f_{X}(x)$:
  \begin{enumerate}
      \item $f_{X}(x)\geq0$ $\lambda$-gairebé arreu
      \item $F_{X}(x)=\int_{(-\infty,x)}f_{x}d\lambda$; $\qquad \int_{\real}f_{X}d\lambda=1$
      \item $p(X=x)=\int_{\setb{x}}f_{X}d\lambda=0 \, \forall x \in \real$
      \item Si $f_{X}$ és integrable Riemann, $F_{X}(x)=\int_{-\infty}^{x}f_{X}(x)dx$ i $\dfrac{dF_{X}(x)}{dx}=f_{X}(x)$
  \end{enumerate}
\end{prop}

\begin{obs}
  Tota funció mesurable $f(x)$ que compleixi: 
  \[
    \int_{\real}fd\lambda=1, \quad f(x)\geq 0 \qquad \lambda \text{-gairebé arreu.}
  \]
  Aleshores $\exists X$ variable aleatòria absolutament contínua per la qual $f(x)=f_{X}(x)$. \\
  En aquest context, tenim:
  \[
    \E[X]= \int_{\Omega}Xdp = \int_{\real}xdp_{X} \overset{R-N}{=}\int_{\real}x\underbrace{f_{X}(x)d\lambda}_{dp_{X}} \underset{si f_{X} int. R.}{=} \int_{-\infty}^{+\infty}xf_{X}(x)dx
  \]
  En particular,
  \[
    \V ar[X] = \int_{-\infty}^{+\infty}x^{2}f_{X}(x)dx - \bigg( \int_{-\infty}^{+\infty}xf_{X}(x)dx \bigg)^{2}
  \]
\end{obs}

\subsection{Models de variables aleatòries absolutament contínues}

\begin{enumerate}
    \item \underline{Uniforme}: $X \sim U([a,b])$ \quad (Tria un nombre uniformement a l'atzar en l'interval $[a,b]$). \\
    \[
      f_{X}(x)=\frac{1}{b-a}\mathbbm{1}_{[a,b]}(x)
    \]
    \[
      \E[X]=\frac{b+a}{2}
    \]
    \[
      \V ar[X]=\frac{(b-a)^{3}}{12}
    \]
    \item \underline{Exponencial}: $X\sim Exp(\lambda)$ \quad (S'usa per modelar el tipus de vida d'un aparell. És l'anàleg continu de la Poisson).
    \[
      f_{X}(x)=\lambda\cdot e^{-\lambda\cdot x}\cdot \mathbbm{1}_{[0,+\infty)}(x)
    \]
    \[
      \E[X]=\frac{1}{\lambda}
    \]
    \[
      \V ar[X]=\frac{1}{\lambda^{2}}
    \]
    \item \underline{Normal}: $X\sim N(\mu, \sigma^{2})$ \quad ($\mu \in \real, \, \sigma > 0$)
    \[
      f_{X}(x)=\frac{1}{\sqrt{2\pi\sigma^{2}}}\cdot e^{-\frac{(x-\mu)^{2}}{2\sigma^{2}}}
    \]
    \[
      \E[X]=\mu
    \]
    \[
      \V ar[X]=\sigma^{2}
    \]
    \item \underline{Gamma}: $X \sim \Gamma(\lambda, \tau)$
    \[
      f_{X}(x)=\frac{1}{\Gamma(\tau)}\cdot \lambda^{\tau}\cdot x^{\tau -1}\cdot e^{-\lambda x}\cdot \mathbbm{1}_{[0,+\infty)}(x)
    \]
    \[
      \E[X]=\frac{\tau}{\lambda}
    \]
    \[
      \V ar[X]=\frac{\tau}{\lambda^{2}}
    \]
    \item \underline{Weibull}: $X\sim Weib(\alpha, \beta)$
    \[
      f_{X}(x)=\alpha\cdot \beta \cdot x^{\beta-1}\cdot e^{-\alpha\cdot x^{b}}\cdot \mathbbm{1}_{[0,+\infty)}(x)
    \]
    \[
      \E[X]=\alpha^{-\frac{1}{\beta}}\cdot \Gamma(1+\frac{1}{\beta})
    \]
    \[
      \V ar[X]=\alpha^{-\frac{2}{\beta}}(\Gamma(1+\frac{2}{\beta})-\Gamma^{2}(1+\frac{1}{\beta}))
    \]
    \item \underline{Beta}: $X\sim \beta(a,b)$
    \[
      f_{X}(x)=\frac{\Gamma(a+b)}{\Gamma(a)\cdot \Gamma(b)}\cdot x^{a-1}\cdot (1-x)^{b-1}\cdot \mathbbm{1}_{[0,1]}(x)
    \]
    \[
      \E[X]=\frac{a}{a+b}
    \]
    \[
      \V ar[X]=\frac{a\cdot b}{(a+b)^{2}\cdot (a+b+1)}
    \]
    \item \underline{Cauchy}: $X\sim Cauchy$
    \[
      f_{X}(x)=\frac{1}{\pi\cdot (1+x^{2})} \quad (x \geq 0)
    \]
     \quad Cap dels $\E[X^{k}]$ és finit.
\end{enumerate}

\subsection{Distribucions conjuntes i marginals. Independència i distribucions condicionades}

Ara farem el mateix que per una variable en el cas de tenir vectors de variables aleatòries.\\
Ho farem per vectors $(X_{1},X_{2})$, però és fàcilment generalitzable a vectors de dimensió $> 2$. \\
Sigui $(X,Y)$ un vector de variables aleatòries. $(X,Y)$ indueix $p_{(X,Y)}$ mesura de probabilitat en $\real^{2}$.

\begin{defi}
  $(X,Y)$ és un \textbf{vector absolutament continu} si $p_{(X,Y)}<<\lambda_{\real^{2}}$. \\
  Per $Radon-Nikodym$, $(X,Y)$ té una \textbf{funció de densitat}:
  \[
    B\in\mathcal{B}_{\real^{2}}, \, p_{(X,Y)}(B)=\iint_{B}f_{(X,Y)}d\lambda_{\real^{2}}
  \]
\end{defi}

\begin{defi}
  $f_{X,Y}(x,y)$ és la \textbf{funció de densitat conjunta} de $X$ i $Y$.\\
  Si ara $B=(a_{1},a_{2})\times(b_{1},b_{2})$ i $f_{(X,Y)}(x,y)$ és integrable Riemann, 
  \[
    p_{(X,Y)}(B)=\iint_{B}f_{(X,Y)}\,d\lambda_{\real^{2}}=\int_{a_{1}}^{a_{2}}\int_{b_{1}}^{b_{2}}f_{(X,Y)}(x,y)\,dy\,dx
  \]
\end{defi}

A partir de $f_{(X,Y)}(x,y)$ podem extreure la funció de densitat de $X$ i de $Y$ integrant:

\begin{defi}
  Donat un vector de variables aleatòries $(X,Y)$ amb funció de densitat conjunta $f_{(X,Y)}(x,y)$, les \textbf{funcions de densitat marginals} són:
  \[
    f_{X}(x) = \int_{-\infty}^{+\infty}f_{(X,Y)}(x,y) \, dy
  \]
  \[
    f_{Y}(y) = \int_{-\infty}^{+\infty}f_{(X,Y)}(x,y) \, dx
  \]
\end{defi}

A partir de la funció de densitat conjunta podem definir la \textbf{funció de distribució}: 
\[
  F_{(X,Y)}(x,y) = P(X \leq x, Y \leq y) \underset{int. Riem.}{=} \int_{-\infty}^{x}\int_{-\infty}^{y} f_{(X,Y)}(x,y) \, dy \, dx
\]

\begin{obs}
  Si $f_{X,Y}(x,y)$ és integrable Riemann $\implies F_{(X,Y)}(x,y)$ és $\mathcal{C}^{2}(\real^{2})$ i
  \[
    \frac{\partial^{2}F_{(X,Y)}}{\partial x \partial y}(x,y) = \frac{\partial^{2}F_{(X,Y)}}{\partial y \partial x}(x,y) = f_{(X,Y)}(x,y)
  \]
\end{obs}

\begin{obs}
  Si $g:\real^{2} \to \real$ és una funció mesurable, $g(X,Y)$ és una variable aleatòria i es compleix:
  \[
    \E[g(X,Y)] = \iint_{\real^{2}} g(x,y)\cdot f_{(X,Y)}(x,y) \, dx \, dy
  \]
\end{obs}


\subsubsection{Independència}

Si $(X,Y)$ és un vector aleatori i $X$, $Y$ són independents, aleshores $F_{X,Y}(x,y) = F_{X}(x)\cdot F_{Y}(y)$. Ara si $F_{(X,Y)}$ és $\mathcal{C}^{2}$ i les $F_{X}$, $F_{Y}$ són derivables (això sempre passa en el cas absolutament continuu), llavors

\[
  f_{(X,Y)}(x,y) = \frac{\partial^{2}F_{(X,Y)}}{\partial x \partial y} = \frac{\partial F_{X}}{\partial x}\cdot \frac{\partial F_{Y}}{\partial y} = f_{X}(x) \cdot f_{Y}(y)
\]

Per tant, $X$ i $Y$ variables aleatòries absolutament contínues són independents sii
\[
  f_{(X,Y)}(x,y) = f_{X}(x)\cdot f_{Y}(y)
\]

\underline{Notació}: si $(X,Y)$ és un vector aleatori, 
\[
  \E[(X,Y)] = (\E[X], \E[Y])
\]

\[
  \V ar[(X,Y)] = 
  \begin{pmatrix}
    \V ar[X]  &  Cov(X,Y) \\
    Cov(X,Y)  &  \V ar[Y]
  \end{pmatrix}
  \underset{Si X,\, Y ind}{=}
  \begin{pmatrix}
    \V ar[X]  &  0 \\
    0         &  \V ar[Y]
  \end{pmatrix}
\]

\subsubsection{Distribucions condicionades}

Siguin $X$, $Y$ variables aleatòries absolutament contínues, amb funció de densitat conjunta $f_{(X,Y)}(x,y)$ i marginals $f_{X}(x)$, $f_{Y}(y)$. Sigui $x$ tal que $f_{X}(x) > 0$.

\begin{defi}
  La veriable aleatòria $Y \mid X = x$ és una variable aleatòria que té com a funció de distribució:
  \[
    F_{Y\mid X}(y,x) = \frac{1}{f_{X}(x)} \cdot \int_{-\infty}^{y}f_{(X,Y)}(x,u) \, du
  \]
  Si $f_{(X,Y)}(x,y)$ és integrable respecte a y, aleshores, $F_{Y \mid X}(y,x)$ és derivable respecte a $y$, i es compleix:
  \[
    f_{Y\mid X}(y,x) = \frac{\partial F_{Y\mid X}(y,x)}{\partial y} = \frac{f_{(X,Y)(x,y)}}{f_{X}(x)}
  \]
\end{defi}

\subsubsection{Esperança condicionada}

En les condicions d'abans, definim:
\[
  \E[Y\mid X=x] = \int_{-\infty}^{+\infty}u\cdot f_{Y\mid X}(u,x)\, du = \psi(x) \implies \E[Y\mid X] = \psi(X)
\]

Totes les propietats que vam veure en el cas discret per l'esperança condicionada s'apliquen aquí de la mateixa manera. En particular:

\begin{prop}
  \[
    \E\big[\,\E[Y\mid X]\,\big] = \E[Y] \qquad \text{(si $(X,Y)$ és abs. cont.)}
  \]
\end{prop}

\subsection{Funcions de v.a. absolutament contínues i aplicacions}

Sigui $\overrightarrow{X} = (X_{1}, \ldots , X_{n})$ un vector aleatori absolutament 
continuu i $G:\real^{n} \to \real^{n}$ una funció bijectiva. \\

Com relacionem $f_{(X_{1},\ldots, X_{n})}(x_{1},\ldots,x_{n})$ amb $f_{(Y_{1},\ldots, Y_{n})}(y_{1},\ldots,y_{n})$?

\subsubsection{Cas univariat}
Sigui $X$ una variable aleatòria absolutament contínua amb funció de densitat $f_{X}(x)$. Sigui $g:\real \to \real$ una funció bijectiva, derivable i estrictament creixent. Considerem ara $g(X) = Y$ (i $\inv{g} = h$). Aleshores,

\[f_{Y}(u) \text{ i } f_{X}(h(u))\cdot h'(u) \text{ són iguals } \lambda-gaireb\acute{e} \, per \, tot\]

\begin{obs}
  Si $g$ no és bijectiva o no és estrictament creixent, en general l'anàlisi és més complicat.
\end{obs}

\subsubsection{Cas multivariat}
$G(\overrightarrow{X}) = \overrightarrow{Y}$ on $G$ és bijectiva i $\mathcal{C}^{1}(\real^{n})$, aleshores, de manera similar al cas anterior, tenim:
\[
  f_{(Y_{1},\ldots,Y_{n})}(y_{1},\ldots,y_{n}) = f_{(X_{1},\ldots,X_{n})}(\inv{G}(y_{1},\ldots,y_{n}))\cdot \abs{Jac\,\inv{G}(y_{1},\ldots,y_{n})}
\]

\subsection{Distribució normal, multivariant i distribucions associades}

Ja vam veure que la distribució normal ve donada per la seva esperança i la seva variància: 
\[
  X \sim N(\mu, \sigma^{2})
\]
\[
  f_{X}(x) = \frac{1}{\sqrt{2\pi\sigma^{2}}}\cdot e^{\frac{-1}{2\sigma}\cdot(x-\mu)^{2}}
\]

La suma de normals independents és normal.

\begin{thm}[Moivre-Laplace]
  Sigui $X\sim Bin(n,p)$. Aleshores 
  \[
    p\Bigg(a \leq \frac{X-np}{\sqrt{np(1-p)}}\leq b\Bigg)\xrightarrow[n\to\infty]{} \int_{b}^{a} \frac{1}{\sqrt{2\pi}}\cdot e^{-\frac{x^{2}}{2}}\, dx
  \]
\end{thm}

\newpage

\subsubsection{Distribucions associades a la normal}
\begin{itemize}
    \item $X_{i}\sim N(0,1)$; $X_{1}, \, X_{2},\ldots$ independents. 
    \[
      X_{1}^{2}+X_{2}^{2}+\ldots+X_{n}^{2}=\chi_{n}^{2} \quad \equiv \textbf{Chi quadrat} \text{ (amb n graus de llibertat})
    \]
    \item Si tenim $\chi_{d_{1}}^{2}$ i $\chi_{d_{2}}^{2}$, aleshores
    \[
      \frac{^{\chi_{d_{1}}^{2}} / _{d_{1}}}{^{\chi_{d_{2}}^{2}} / _{d_{2}}} \equiv \textbf{F-Fisher-Snedecor} \text{ (amb paràmetres $d_{1}$ i $d_{2}$)}
    \]
    \item $X\sim N(0,1)$, $\chi_{k}^{2}$ independent de $X$.
    \[
      \frac{X}{\sqrt{^{\chi_{k}^{2}} / _k}} \equiv \textbf{t - de Student} \text{ (amb $k$ graus de llibertat)}
    \]
\end{itemize}

\subsubsection{Normal multivariant}

\begin{defi}
  Sigui $\overrightarrow{X}=(X_{1},\ldots,X_{n})$ un vector aleatori, direm que $\overrightarrow{X}$ és un \textbf{vector de variables aleatòries normal multivariant} si la seva funció de densitat conjunta és:
  \[
    f_{\overrightarrow{X}}(x_{1},\ldots,x_{n}) = f_{\overrightarrow{X}}(\overrightarrow{x}) = \frac{1}{\sqrt{(2\pi)^{n}\cdot \abs{det(\Sigma)}}}\cdot e^{-\frac{1}{2}(\overrightarrow{x}-\overrightarrow{\mu})^{T}\cdot \inv{\Sigma} \cdot (\overrightarrow{x}-\overrightarrow{\mu})}
  \]
  on
  \begin{itemize}
      \item  $\overrightarrow{\mu}$ és un vector de $\real^{n}$ (vector d'esperances)
      \item $\Sigma$ és una matriu $n\times n$ simètrica i definida positiva (matriu de covariàncies)
  \end{itemize}
  \[
    \overrightarrow{\mu} = \E[\overrightarrow{x}]
  \]
  \[
    \Sigma = \Big( Cov(x_{i},x_{j})\Big)_{i,j}
  \]
\end{defi}

\underline{Cas particular}: Si $\Sigma = Id$, $\overrightarrow{\mu} = \overrightarrow{0}$, aleshores escriurem $\overrightarrow{X}=\overrightarrow{U}$ i es compleix que 
\[
f_{\overrightarrow{U}}(\overrightarrow{x}) = \frac{1}{\sqrt{(2\pi)^{n}}}\cdot e^{-\frac{1}{2}\cdot(\overrightarrow{x}-\overrightarrow{\mu})^{T}\cdot  (\overrightarrow{x}-\overrightarrow{\mu})} = \frac{1}{\sqrt{(2\pi)^{n}}}\cdot e^{-\frac{1}{2}\cdot\sum\limits_{i=1}^{n}x_{i}^{2}} = \prod_{i=1}^{n} f_{x_{i}}(x_{i})
\]
Per tant, les marginals són normals univariades i independents. \\

Ara veurem que tot vector normal multivariant s'obté de fet com una transformació lineal de $\overrightarrow{U}$.

\begin{thm}
  $\overrightarrow{X}=(X_{1},\ldots,X_{n})$ segueix una llei normal multivariant $\iff \overrightarrow{X} = A\cdot\overrightarrow{X} + \overrightarrow{b}$ on $A$ és una matriu \underline{no} singular ($\Sigma = A^{T}\cdot diag \cdot A$).
\end{thm}

\begin{thm}
  Sigui $\overrightarrow{X}\sim N(\overrightarrow{\mu}, \Sigma)$ de dimensió $n$, i $M$ una matriu $m\times n$ de rang màxim ($m\leq n$). Aleshores $M\cdot\overrightarrow{X}$ és un vector de variables aleatòries normal multivariant $N(M\cdot\overrightarrow{\mu}, M\cdot\Sigma\cdot M^{T})$.
\end{thm}
\-\\
En el cas de $m = 1$ tenim el següent corol·lari:

\begin{col}
  Si $a_{1},\ldots,a_{n}$ són nombres tals que $\sum a_{i}^2 > 0$ (la matriu $(a_{1},\ldots,a_{n})$ té rang 1) i $\overrightarrow{X}\sim N(\overrightarrow{\mu}, \Sigma)$, $\overrightarrow{X} = (X_{1},\ldots,X_{n})$
  \[
    M\overrightarrow{X} = \sum_{i=1}^{n} a_{i}x_{i} \sim N\bigg(\sum_{i=1}^{n} a_{i}\mu_{i}, \sum a_{i}^{2}\sigma_{i}^{2} + 2\sum_{i<j}a_{i}a_{j}\sigma_{ij}\bigg) \quad \text{ on } 
    \begin{cases}
      \overrightarrow{\mu} = (\mu_{1},\ldots,\mu) \\
      \sigma_{i}^{2} = \V ar[X_{i}] \\
      \sigma_{ij} = Cov(X_{i}, X_{j})
    \end{cases}
  \]
\end{col}

\-\\
Seguidament veurem els estimadors i el teorema de Fisher:

\begin{defi}
  Siguin $X_{1}, X_{2}, \ldots, X_{n}$ variables aleatòries idènticament distribuïdes. L'\textbf{esperança mostral} i la \textbf{variància mostral} són: \\
  \[
    \overline{X} = \frac{X_{1}+\ldots+X_{n}}{n}
  \]
  \[
    S^{2} = \frac{1}{n-1}\cdot\sum_{i=1}^{n}(x_{i} - \overline{X})^{2}
  \]
  \-\\
  En particular, $\E[\overline{X}] = \E[X_{1}]$, $\V ar[\overline{X}] = \frac{\V ar[X_{1}]}{n}$ i $\E[S^{2}] = \V ar[X_{1}]$ \\
  
\end{defi}

Si, a més, $X_{1},\ldots,X_{n}$ són $N(\mu,\sigma^{2})$, tenim el següent teorema:

\begin{thm}[Fisher]
  Si $X_{1},\ldots, X_{n}$ són independents i $N(\mu, \sigma^{2})$, aleshores $\overline{X}$ i $S^{2}$ són independents. \\ 
  A més, $\overline{X} \sim N(\mu, \frac{\sigma^{2}}{n})$ i $S^{2} \sim \chi_{n-1}^{2}$
\end{thm}
